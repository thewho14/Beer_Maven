{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scrapping ratings from beeradvocate.com<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import urllib2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import bs4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code covers steps I took in creating Beer Maven.  Beer Maven is a beer recommendation system that allows users to search for beers and add them to a custom beer list.  This list is then sorted in the order a user will enjoy the beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First go to the top 250 users webpage on beer advocate and parse it into a bs4 object\n",
    "res = requests.get('https://www.beeradvocate.com/members/?sort=beers')\n",
    "Soup = bs4.BeautifulSoup(res.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/community/members/stonedtrippin.601042/',\n",
       " '/community/members/stonedtrippin.601042/',\n",
       " '/community/members/uclabrewn84.439438/',\n",
       " '/community/members/uclabrewn84.439438/',\n",
       " '/community/members/sammy.3853/',\n",
       " '/community/members/sammy.3853/',\n",
       " '/community/members/biboergosum.168458/',\n",
       " '/community/members/biboergosum.168458/',\n",
       " '/community/members/beerchitect.14442/',\n",
       " '/community/members/beerchitect.14442/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the url's for each individual user\n",
    "urls = [link.get('href') for link in Soup.find_all(class_='username')]\n",
    "urls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10173,\n",
       " 8767,\n",
       " 7737,\n",
       " 7193,\n",
       " 7146,\n",
       " 6852,\n",
       " 6581,\n",
       " 6450,\n",
       " 6281,\n",
       " 5810,\n",
       " 5690,\n",
       " 5620,\n",
       " 5315,\n",
       " 5279,\n",
       " 5159,\n",
       " 5158,\n",
       " 5058,\n",
       " 4912,\n",
       " 4866,\n",
       " 4796,\n",
       " 4789,\n",
       " 4662,\n",
       " 4622,\n",
       " 4290,\n",
       " 4256,\n",
       " 4174,\n",
       " 4021,\n",
       " 3947,\n",
       " 3931,\n",
       " 3886,\n",
       " 3852,\n",
       " 3846,\n",
       " 3830,\n",
       " 3819,\n",
       " 3774,\n",
       " 3772,\n",
       " 3722,\n",
       " 3718,\n",
       " 3715,\n",
       " 3712,\n",
       " 3647,\n",
       " 3606,\n",
       " 3594,\n",
       " 3590,\n",
       " 3543,\n",
       " 3517,\n",
       " 3398,\n",
       " 3388,\n",
       " 3313,\n",
       " 3297,\n",
       " 3293,\n",
       " 3284,\n",
       " 3212,\n",
       " 3181,\n",
       " 3155,\n",
       " 3153,\n",
       " 3145,\n",
       " 3117,\n",
       " 3116,\n",
       " 3102,\n",
       " 3099,\n",
       " 3038,\n",
       " 2983,\n",
       " 2885,\n",
       " 2863,\n",
       " 2852,\n",
       " 2807,\n",
       " 2750,\n",
       " 2703,\n",
       " 2666,\n",
       " 2638,\n",
       " 2635,\n",
       " 2635,\n",
       " 2629,\n",
       " 2629,\n",
       " 2591,\n",
       " 2586,\n",
       " 2559,\n",
       " 2521,\n",
       " 2502,\n",
       " 2500,\n",
       " 2494,\n",
       " 2492,\n",
       " 2486,\n",
       " 2465,\n",
       " 2465,\n",
       " 2463,\n",
       " 2455,\n",
       " 2454,\n",
       " 2416,\n",
       " 2369,\n",
       " 2340,\n",
       " 2330,\n",
       " 2330,\n",
       " 2324,\n",
       " 2289,\n",
       " 2266,\n",
       " 2235,\n",
       " 2232,\n",
       " 2200,\n",
       " 2197,\n",
       " 2179,\n",
       " 2164,\n",
       " 2131,\n",
       " 2094,\n",
       " 2092,\n",
       " 2090,\n",
       " 2078,\n",
       " 2063,\n",
       " 2053,\n",
       " 2051,\n",
       " 2049,\n",
       " 2048,\n",
       " 2044,\n",
       " 2039,\n",
       " 2039,\n",
       " 2032,\n",
       " 2000,\n",
       " 1996,\n",
       " 1996,\n",
       " 1994,\n",
       " 1993,\n",
       " 1955,\n",
       " 1953,\n",
       " 1947,\n",
       " 1946,\n",
       " 1945,\n",
       " 1939,\n",
       " 1927,\n",
       " 1906,\n",
       " 1900,\n",
       " 1887,\n",
       " 1864,\n",
       " 1845,\n",
       " 1842,\n",
       " 1839,\n",
       " 1830,\n",
       " 1820,\n",
       " 1819,\n",
       " 1819,\n",
       " 1818,\n",
       " 1815,\n",
       " 1808,\n",
       " 1805,\n",
       " 1799,\n",
       " 1772,\n",
       " 1755,\n",
       " 1744,\n",
       " 1740,\n",
       " 1737,\n",
       " 1723,\n",
       " 1721,\n",
       " 1718,\n",
       " 1717,\n",
       " 1713,\n",
       " 1711,\n",
       " 1685,\n",
       " 1685,\n",
       " 1677,\n",
       " 1667,\n",
       " 1660,\n",
       " 1632,\n",
       " 1632,\n",
       " 1627,\n",
       " 1619,\n",
       " 1616,\n",
       " 1610,\n",
       " 1602,\n",
       " 1586,\n",
       " 1584,\n",
       " 1579,\n",
       " 1573,\n",
       " 1570,\n",
       " 1560,\n",
       " 1554,\n",
       " 1553,\n",
       " 1544,\n",
       " 1541,\n",
       " 1512,\n",
       " 1503,\n",
       " 1499,\n",
       " 1491,\n",
       " 1489,\n",
       " 1487,\n",
       " 1473,\n",
       " 1445,\n",
       " 1442,\n",
       " 1440,\n",
       " 1435,\n",
       " 1431,\n",
       " 1426,\n",
       " 1421,\n",
       " 1420,\n",
       " 1419,\n",
       " 1401,\n",
       " 1399,\n",
       " 1386,\n",
       " 1381,\n",
       " 1379,\n",
       " 1374,\n",
       " 1372,\n",
       " 1368,\n",
       " 1364,\n",
       " 1359,\n",
       " 1358,\n",
       " 1345,\n",
       " 1343,\n",
       " 1339,\n",
       " 1337,\n",
       " 1335,\n",
       " 1323,\n",
       " 1318,\n",
       " 1316,\n",
       " 1306,\n",
       " 1292,\n",
       " 1289,\n",
       " 1284,\n",
       " 1282,\n",
       " 1280,\n",
       " 1277,\n",
       " 1272,\n",
       " 1261,\n",
       " 1259,\n",
       " 1249,\n",
       " 1242,\n",
       " 1239,\n",
       " 1237,\n",
       " 1236,\n",
       " 1233,\n",
       " 1224,\n",
       " 1212,\n",
       " 1198,\n",
       " 1183,\n",
       " 1183,\n",
       " 1180,\n",
       " 1180,\n",
       " 1179,\n",
       " 1172,\n",
       " 1168,\n",
       " 1167,\n",
       " 1154,\n",
       " 1147,\n",
       " 1143,\n",
       " 1132,\n",
       " 1130,\n",
       " 1130,\n",
       " 1130,\n",
       " 1122,\n",
       " 1117,\n",
       " 1115]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of reviews for each user and remove extra junk\n",
    "numb = []\n",
    "for n in Soup.find_all(class_='username'):\n",
    "    numb.extend(n.find_next().find_next().contents)\n",
    "\n",
    "numb = [int(n.replace(',','')) for n in numb[:250]]\n",
    "numb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/community/members/stonedtrippin.601042/',\n",
       " '/community/members/uclabrewn84.439438/',\n",
       " '/community/members/sammy.3853/',\n",
       " '/community/members/biboergosum.168458/',\n",
       " '/community/members/beerchitect.14442/',\n",
       " '/community/members/metter98.95017/',\n",
       " '/community/members/brentk56.6284/',\n",
       " '/community/members/phyl21ca.2335/',\n",
       " '/community/members/superspak.456300/',\n",
       " '/community/members/nerofiddled.526/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplicates and urls grab erroneously\n",
    "urls = [n for n in urls if n[0]=='/']\n",
    "urls = urls[::2]\n",
    "urls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stonedtrippin',\n",
       " 'uclabrewn84',\n",
       " 'sammy',\n",
       " 'biboergosum',\n",
       " 'beerchitect',\n",
       " 'metter98',\n",
       " 'brentk56',\n",
       " 'phyl21ca',\n",
       " 'superspak',\n",
       " 'nerofiddled']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract usernames from urls\n",
    "users = [r.replace('/community/members/','') for r in urls]\n",
    "users = [r.rsplit('.',1)[0] for r in users]\n",
    "users[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.beeradvocate.com/user/beers/?ba=stonedtrippin',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=uclabrewn84',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=sammy',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=biboergosum',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=beerchitect',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=metter98',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=brentk56',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=phyl21ca',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=superspak',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=nerofiddled']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create beer review url with username to find all beer rated by this user\n",
    "user_links = ['https://www.beeradvocate.com/user/beers/?ba='+ k for k in users]\n",
    "user_links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=0',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=50',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=100',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=150',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=200',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=250',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=300',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=350',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=400',\n",
       " 'https://www.beeradvocate.com/user/beers/?ba=stonedtrippin&&start=450']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add starting page number to access each page of reviews.  calculate that using total number of review/50 (50 per page)\n",
    "page_links = []\n",
    "for n in range(len(users)):\n",
    "    for k in range(0,numb[n],50):\n",
    "        page_links.append(user_links[n]+'&&start='+str(k))\n",
    "        \n",
    "page_links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize empty dataframe\n",
    "df = pd.DataFrame(columns = ['username','beer','rating','rDev'])\n",
    "ind = -1\n",
    "for link in range(len(page_links)):\n",
    "    \n",
    "    #This part counts when page url's restart at 0 aka when we've moved on to the next user\n",
    "    if page_links[link][-2:] == '=0':\n",
    "        ind = ind + 1\n",
    "    try:\n",
    "        #Parse each webpage into bs4\n",
    "        rest = requests.get(page_links[link])\n",
    "        souper = bs4.BeautifulSoup(rest.text,'lxml')\n",
    "        \n",
    "        #Get rating and rDev and add in username\n",
    "        stuff = souper.find_all('b')[5:]\n",
    "        beer_names = np.array([k.contents[0] for k in stuff[:-1:2]])\n",
    "        ratings = np.array([p.contents[0] for p in stuff[1::2]])\n",
    "        rDev = np.array([r.find_next().text for r in stuff[1::2]])\n",
    "        col1 = np.repeat(users[ind],len(ratings))\n",
    "        \n",
    "        #Get beer and brewery names\n",
    "        arc = souper.find_all('a')\n",
    "        duck = np.array([arc[n].contents for n in range(len(arc)) if str(arc[n].get('href'))[0:14]=='/beer/profile/'][1::2])\n",
    "        \n",
    "        #Add to the empty dataframe\n",
    "        mat = pd.DataFrame(np.column_stack([col1,beer_names,duck,ratings,rDev]),columns = ['username','beer','brewery','rating','rDev'])\n",
    "        df = pd.concat([df,mat],ignore_index=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>beer</th>\n",
       "      <th>rating</th>\n",
       "      <th>rDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [username, beer, rating, rDev]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels [104081] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-60cf6f9d43f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rDev'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rDev'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rDev'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrDev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m104081\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m103930\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Beer_Rating_Dataset.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zhutk\\Anaconda2\\Lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[0;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2161\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zhutk\\Anaconda2\\Lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[1;32m-> 3624\u001b[1;33m                                  labels[mask])\n\u001b[0m\u001b[0;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels [104081] not contained in axis"
     ]
    }
   ],
   "source": [
    "#basic data cleaning and saving to pkl file\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "df.loc[:,'rating'] = pd.to_numeric(df.rating)\n",
    "df.loc[:,'rDev'] = df.loc[:,'rDev'].str.replace('%','').str.replace('+','')\n",
    "df.loc[:,'rDev'] = pd.to_numeric(df.rDev)\n",
    "df.drop(104081, inplace=True)\n",
    "df.drop(103930, inplace=True)\n",
    "df = pickle.dump(df,open(\"Beer_Rating_Dataset.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign each unique beer, brewery combination an id number\n",
    "df = df.assign(beer_id=(df['brewery'] + '_' + df['beer']).astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scraping Beer Types<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Go to beer styles page\n",
    "res = requests.get('https://www.beeradvocate.com/beer/style/')\n",
    "Soup = bs4.BeautifulSoup(res.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get urls\n",
    "urls = [link.get('href') for link in Soup.find_all('a')]\n",
    "urls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#throw away bad urls\n",
    "good = [x for x in urls if type(x)==str]\n",
    "good[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter other bad urls\n",
    "better = [x for x in good if x[6:11]=='style']\n",
    "better[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get ride of bad links at the start and end of list\n",
    "bet = better[2:-2]\n",
    "bet[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get number of reviews.  The number is listed several ways and so the loop needs a way to catch all of them.\n",
    "numbs = []\n",
    "for r in range(len(bet)):\n",
    "    try:\n",
    "        res = requests.get(bet[r])\n",
    "        ba = bs4.BeautifulSoup(res.text,'lxml')\n",
    "        if ba.find_all('b')[3].contents[0][0:5]=='Style':\n",
    "            numb = ba.find_all('b')[3].contents[0].split('of ')[1].split(') -')[0]\n",
    "            numbs.append(int(numb))\n",
    "        else:\n",
    "            numb = ba.find_all('b')[2].contents[0].split('of ')[1].split(') -')[0]\n",
    "            numbs.append(int(numb))\n",
    "    except:\n",
    "        if ba.find_all('b')[4].contents[0][0:5]=='Style':\n",
    "                numb = ba.find_all('b')[4].contents[0].split('of ')[1].split(') -')[0]\n",
    "                numbs.append(int(numb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add page number to end of urls\n",
    "page_links = []\n",
    "for n in range(len(bet)):\n",
    "    for k in range(0,numbs[n],50):\n",
    "        page_links.append(bet[n]+str(k))\n",
    "        \n",
    "page_links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get beer styles and names\n",
    "dg = pd.DataFrame(columns=['brewery','beer','type'])\n",
    "for link in range(len(page_links)):\n",
    "        try:\n",
    "            ttt = []\n",
    "            grr = []\n",
    "            print (float(link)/float(4000))*100\n",
    "            rest = requests.get(page_links[link])\n",
    "            souper = bs4.BeautifulSoup(rest.text,'lxml')\n",
    "            category = souper.find_all('h1')[0].contents[0]\n",
    "            beers = souper.find_all('b')[5:-1:3]\n",
    "            beers = [x.contents[0] for x in beers]\n",
    "            dab = [ba.find_all('a')[x].contents for x in range(len(ba.find_all('a')))]\n",
    "            for x in dab:\n",
    "                try:\n",
    "                    if type(x[0]) == bs4.element.NavigableString:\n",
    "                        grr.append(x[0])\n",
    "                        ttt = grr[124:-13]\n",
    "                except:\n",
    "                    pass\n",
    "            place = np.column_stack([ttt, beers])\n",
    "            dh = pd.DataFrame(place,columns = ['brewery', 'beer'])\n",
    "            dh.loc[:,'type'] = category\n",
    "            dg = pd.concat([dg,dh],ignore_index=True)\n",
    "        except:\n",
    "            print page_links[link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exploratory Data Analysis<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Beer_Dataset')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rating.hist()\n",
    "plt.show()\n",
    "print 'The mean of ratings is {} and the standard deviation is {}.'.format(df.rating.mean(),df.rating.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings between 2.5-4.5 are very frequent.  The 5 point rating scale is compressed in this way, such that atrocious beers like Budweiser and Coors get about 2.5 average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The best beers with a minimum of 10 ratings.  Beer with 1/2 reviews sometimes get perfect 5's but this is unreliable\n",
    "multi = mult[mult>=10].index\n",
    "df[df.beer_id.isin(multi)].groupby(['beer_id','brewery','beer']).mean().sort_values('rating',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The worst beers with a minimum of 10 ratings\n",
    "df[df.beer_id.isin(multi)].groupby(['beer_id','brewery','beer']).mean().sort_values('rating').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#average ratings for beer group\n",
    "df.groupby('group').mean().rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby('group').rating.mean().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard deviation of groups.  Higher standard deviation means a more controversial beer\n",
    "df.groupby('group').rating.std().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rDev is rating deviation.  It represents how far of the website wide average rating a user was.  Here it represents how\n",
    "#top 250 reviewers rated the beer in comparison to every user.\n",
    "df.groupby('group').rDev.mean().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need more stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assigning Beer Groups<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import surprise\n",
    "#load rating datafram\n",
    "df = pickle.load(open('Beer_Rating_Dataset.pkl','rb'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load beer styles\n",
    "dg = pickle.load(open('beer_types_final.pkl','rb'))\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find beers with more than one style listed\n",
    "recur = dg.beer.value_counts().values > 1\n",
    "tulips = dg.beer.value_counts()[recur].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign the most common beer style listed to beer with more than one style.  If there is a tie the first alphabetically is picked\n",
    "for n in tulips:\n",
    "    foam = dg[dg.beer==n].type\n",
    "    dg.loc[n,'type'] = foam.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge dataframes\n",
    "dz = df.merge(dg,on='beer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lack of a better option, groups were assigned by me for the purpose learning user preference.  In the future I would love to have beer experts create their own groups and average them.  I tried to do this in a post on beeradvocate.com but got banned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPA = ['American IPA','American Pale Ale (APA)','American Double / Imperial IPA','English India Pale Ale (IPA)',\n",
    "       'Belgian IPA','English Pale Ale','Belgian Pale Ale','American Black Ale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DarkAles = ['American Amber / Red Ale', 'American Brown Ale','Belgian Dark Ale','English Brown Ale',\n",
    "            'Irish Red Ale','Flanders Red Ale','Altbier',u'Bière de Garde','Scottish Ale','Dubbel',\n",
    "            'Quadrupel (Quad)','Winter Warmer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PorterStouts = ['American Porter','American Stout','American Double / Imperial Stout','English Porter','English Stout',\n",
    "                'Milk / Sweet Stout','Irish Dry Stout','Foreign / Export Stout','Oatmeal Stout','Russian Imperial Stout',\n",
    "                'Baltic Porter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wheat = ['American Dark Wheat Ale','American Pale Wheat Ale','American Blonde Ale','Witbier','English Pale Mild Ale',\n",
    "         'English Dark Mild Ale','Berliner Weissbier','Dunkelweizen','Gose',u'Kölsch','Kölsch','Hefeweizen','Kristalweizen',\n",
    "         'Weizenbock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Strong = ['American Barleywine','Wheatwine','English Strong Ale','English Barleywine','Braggot','Belgian Strong Dark Ale',\n",
    "          'Belgian Strong Pale Ale','American Strong Ale','Euro Strong Lager','American Malt Liquor','Scotch Ale / Wee Heavy',\n",
    "         u'Bière de Champagne / Bière Bru','Tripel',u'Bière de Champagne / Bière Brut','Old Ale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LightLager = ['American Adjunct Lager','American Double / Imperial Pilsner','Light Lager','Czech Pilsener',\n",
    "             'Euro Pale Lager','Dortmunder / Export Lager','German Pilsener','Kellerbier / Zwickelbier','Munich Helles Lager',\n",
    "             'Happoshu','Japanese Rice Lager']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DarkLager = ['Bock','Doppelbock','Eisbock','California Common / Steam Beer','American Amber / Red Lager','American Pale Lager',\n",
    "            'Euro Dark Lager','Maibock / Helles Bock', u'Märzen / Oktoberfest','Munich Dunkel Lager','Rauchbier','Schwarzbier',\n",
    "             'Vienna Lager']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fruity = ['Fruit / Vegetable Beer','Herbed / Spiced Beer','Smoked Beer','American Wild Ale','Lambic - Fruit',\n",
    "        'Saison / Farmhouse Ale','Cream Ale','Pumpkin Ale','Chile Beer','English Bitter','Extra Special / Strong Bitter (ESB)',\n",
    "          'Kvass','Scottish Gruit / Ancient Herbed Ale','Faro','Flanders Oud Bruin','Gueuze','Lambic - Unblended','Black & Tan',\n",
    "         'Roggenbier','Sahti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign groups\n",
    "for x in IPA:\n",
    "    dz.loc[dz.type==x,'group'] = 'IPA/PA'\n",
    "for x in DarkAles:\n",
    "    dz.loc[dz.type==x,'group'] = 'Dark Ale'\n",
    "for x in PorterStouts:\n",
    "    dz.loc[dz.type==x,'group'] = 'Porter/Stout'\n",
    "for x in Wheat:\n",
    "    dz.loc[dz.type==x,'group'] = 'Wheat'\n",
    "for x in Strong:\n",
    "    dz.loc[dz.type==x,'group'] = 'High ABV'\n",
    "for x in DarkLager:\n",
    "    dz.loc[dz.type==x,'group'] = 'Dark Lager'\n",
    "for x in LightLager:\n",
    "    dz.loc[dz.type==x,'group'] = 'Pale Lager'\n",
    "for x in Fruity:\n",
    "    dz.loc[dz.type==x,'group'] = 'Fruity/Flavored'\n",
    "dz.loc[dz.type=='Low Alcohol Beer','group'] = 'Non-Alcoholic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dz.group.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix weird cases\n",
    "dz.loc[dz.beer.str.contains('IPA')&dz.group.isnull(),'group'] = 'IPA/PA'\n",
    "dz.loc[dz.beer.str.contains('Pale')&dz.group.isnull(),'group'] = 'IPA/PA'\n",
    "dz.loc[dz.beer == 'Ryeday The 13th','group'] = 'High ABV'\n",
    "dz.loc[(dz.type == 'Rye Beer')&dz.group.isnull(),'group'] = 'Fruity/Flavored'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Selection<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to fit a model to the ratings data.  I used a recommendation module called Surprise.  I tested every algorith Surprise offers but will only include code for SVD and KNNBaseline here for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNBaseline as KNNB\n",
    "import surprise\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate\n",
    "from surprise import Reader\n",
    "from surprise import GridSearch\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx = pd.read_csv('Beer_Dataset',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "jaja = Dataset.load_from_df(dx[['username','beer_id','rating']], reader)\n",
    "jaja.split()\n",
    "algo = SVD()\n",
    "algo2 = KNNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#call the evaluate method to see how well the untuned SVD algorithm performs\n",
    "perf = evaluate(algo, jaja, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and the same for the KNNBaseline algorithm\n",
    "perf2 = evaluate(algo2, jaja, measures=['RMSE','MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNNBaseline performs the best so now to tune the model with GridSearch\n",
    "params = {'k':[10,20,30,40,50]}\n",
    "Gs = GridSearch(KNNB,params)\n",
    "Gs.evaluate(jaja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding User Preferences<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cold start problem is a major issue for beer Maven.  Initially my goal was for users to rate a 10 or so common beers so that their rating could be added to the dataframe and predicted on. Here is a function I created to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StartProfile():\n",
    "    ratings = []\n",
    "    beers = ['Blue Moon Belgian White','Negra Modelo','Corona Extra','Guinness Draught','Brown Shugga\\'','Fat Tire Amber Ale','Newcastle Brown Ale','Budweiser','Arrogant Bastard','Sculpin IPA']\n",
    "    brewery = ['Coors Brewing Company','Grupo Modelo S.A. de C.V.','Grupo Modelo S.A. de C.V.','Guinnness Ltd.','Lagunitas Brewing Company','New Belgium Brewing','Heineken Nederland B.V.','Anheuser_Busch','Arrogant Brewing','Ballast Point Brewing Company']\n",
    "    beer_id = [33753,52675,52662,52740,64504,76970,55151,5746,6533,8579]\n",
    "    beer_id = [int(x) for x in beer_id]\n",
    "    user = raw_input('Please enter your username:')\n",
    "    print 'Please rate the following beers (1-5).  If you have not tried a beer, respond with 0'\n",
    "    ratings.append(float(raw_input('Blue Moon')))\n",
    "    ratings.append(float(raw_input('Negra Modelo')))\n",
    "    ratings.append(float(raw_input('Corona Extra')))\n",
    "    ratings.append(float(raw_input('Guinness Draught')))\n",
    "    ratings.append(float(raw_input('Lagunitas Brown Shugga\\'')))\n",
    "    ratings.append(float(raw_input('Fat Tire Amber Ale')))\n",
    "    ratings.append(float(raw_input('Newcastle Brown Ale')))\n",
    "    ratings.append(float(raw_input('Budweiser')))\n",
    "    ratings.append(float(raw_input('Arrogant Bastard Ale')))\n",
    "    ratings.append(float(raw_input('Sculpin IPA')))\n",
    "    \n",
    "    length = len(df)\n",
    "    for n in range(len(ratings)):\n",
    "        avg = df[df.beer_id==beer_id[n]].mean()[1]\n",
    "        length += 1\n",
    "        # Users are assigned the average rating if they have not tried a beer.\n",
    "        if ratings[n] == 0:\n",
    "            df.loc[length,'username'] = user\n",
    "            df.loc[length,'brewery'] = brewery[n]\n",
    "            df.loc[length,'beer'] = beers[n]\n",
    "            df.loc[length,'rating'] = avg\n",
    "            df.loc[length,'beer_id'] = beer_id[n]\n",
    "            df.loc[length,'rDev'] = 0\n",
    "        else:\n",
    "            df.loc[length,'username'] = user\n",
    "            df.loc[length,'brewery'] = brewery[n]\n",
    "            df.loc[length,'beer'] = beers[n]\n",
    "            df.loc[length,'rating'] = ratings[n]\n",
    "            df.loc[length,'beer_id'] = beer_id[n]\n",
    "            df.loc[length,'rDev'] = ((ratings[n]-avg)/avg)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I toyed with the number of beer and which beers to use, but I found that no matter what, the recommendation system was too insensitive to these ratings.  Recommendations were being made according to the mean rating of beers of the dataset and did not reflect user preferences at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically beer preferences are communicated by the varieties of beer a person likes.  For example, many people love hoppy IPA's and Pale Ales.  Others like more drinkable wheat beers or dislike the heavy taste of a Porter or Stout. This motivated me to try and solve my cold start problem by asking users to rate types of beer.  This is the reason I scrapped data on beer styles and subsequently binned these styles into 9 larger groups.  Users ratings of these beer groups are used to modify the dataframe, and then the modified dataframe is predicted on by the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is an early mock-up.  This task is handled by the plotly dash app instead\n",
    "def Create_Pref():\n",
    "    '''Run to create a preference vector to be used with Add_User_Preference function'''\n",
    "    groups = ['IPAs and Pale Ales','Porters and Stouts','Fruity and Flavored Beers','Dark Ales','Wheat Beers',\n",
    "              'High Alcohol Content','Pale Lager','Dark Lager','Non-Alcoholic']\n",
    "    prefs = []\n",
    "    \n",
    "    print 'Rate the following beers groups according the following scale:'\n",
    "    print '1: Strongly Dislike   2: Somewhat Dislike   3: Neutral   4: Somewhat Like  5: Love'\n",
    "    for x in groups:\n",
    "        prefs.append(int(raw_input(x)))\n",
    "        \n",
    "    return prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used in the plotly dash app to modify the dataframe\n",
    "def Add_User_Preference(pref_vect, df):\n",
    "    '''Takes a preference vector and the beer rating dataframe and return a modified dataframe to be predicted on'''\n",
    "    data = df.copy()\n",
    "    \n",
    "    #get group names\n",
    "    grouped = data.group.value_counts().index.values\n",
    "    \n",
    "    for x in range(len(grouped)-1):\n",
    "        \n",
    "        #get the original scores for all beers of given group\n",
    "        score = data[data.group==grouped[x]].rating\n",
    "        \n",
    "        #If group has a rating of 1 subtract .5 rating from every beer in the group\n",
    "        if pref_vect[x] == 1:\n",
    "            data.loc[data.group==grouped[x],'rating'] = score - .5\n",
    "        \n",
    "        #If group has rating of 2 subtract .25\n",
    "        if pref_vect[x] == 2:\n",
    "            data.loc[data.group==grouped[x],'rating'] = score - .25\n",
    "        \n",
    "        #If group has rating of 4 add .25\n",
    "        if pref_vect[x] == 4:\n",
    "            data.loc[data.group==grouped[x],'rating'] = score + .25\n",
    "        \n",
    "        #If group has rating of 5 add .5\n",
    "        if pref_vect[x] == 5:\n",
    "            data.loc[data.group==grouped[x],'rating'] = score + .5\n",
    "        data.loc[data.rating > 5,'rating'] = 5\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tinkered with the + and - values for ratings of 1,2,4, and 5 and found adjustments of .25 and .5 to work best.  However, this needs to be tested further and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now train the model on the modified dataframe\n",
    "dr = Add_User_Preference()\n",
    "data = Dataset.load_from_df(dr[['username','beer_id','rating']], reader)\n",
    "#data.split()\n",
    "trainset = data.build_full_trainset()\n",
    "algo2.train(trainset)\n",
    "#testset = trainset.build_anti_testet()\n",
    "#predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue with the build_anti_testset method in the Surprise module.  The method runs for 30 minutes before exceeding the my RAM and crashing.  This makes little sense as the build_full_testset method works totally fine and runs in under a minute.  Fortunately the evaluate method used earlier serves the same purpose and the scores demonstrate that overfitting is not an issue.  Since I cannot build a testset I decide to train the model using the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is included in the plotly dash app and modified so that a username isn't needed and users add to the list by \n",
    "#searching for beer name and brewery\n",
    "\n",
    "def Sort_Beers(user, beer_list):\n",
    "    '''Take a user name and a list of beer_ids.  Returns the list sorted in order of score predicted by the recommendation system'''\n",
    "    ok = [[x,algo.predict(user, x)[3]] for x in beer_list]\n",
    "    okee = pd.DataFrame(ok,columns=['beer_id','rating'])\n",
    "    do = okee.groupby('beer_id').mean().sort_values('rating',ascending=False).index\n",
    "    return sorted([[dr[dr.beer_id == x].rating.values[0], dr[dr.beer_id==x].beer.values[0]] for x in do])[-1::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Search Engine<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the app, user must be able to quickly search for beers by beer, brewery, or a combination of the two.  I started by trying to create my own search in the form of a spell checker using nltk and spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here is one such attempt lusing edit distance\n",
    "import re\n",
    "Words = list(set(dx.brewery.values))\n",
    "\n",
    "def Super_Lookup(entry):\n",
    "    '''Takes a search query and returns the 3 closest results from the set of brewery names'''\n",
    "    #remove stopwords\n",
    "    stopwords = [' Brewing', ' Brewery',' Company',' Co.',' Tasting Room',' Pub',' &',' Craft',' Brauerei','Berkeley','Works','Co']\n",
    "    brewers = Words\n",
    "    for r in stopwords:\n",
    "        entry = entry.replace(r,'')\n",
    "        brewers = [x.replace(r,'') for x in brewers]\n",
    "    \n",
    "    #tokenize entry and brewers\n",
    "    entry_len =  len(re.findall(r'\\b\\w+\\b', entry))\n",
    "    brewer_len = {brewers[x]:(len(re.findall(r'\\b\\w+\\b', brewers[x])),x) for x in range(len(brewers))}\n",
    "    poss = [(x, brewer_len[x][1]) for x in brewer_len.keys() if brewer_len[x][0]>=entry_len]\n",
    "    \n",
    "    #return closest match with same number of words\n",
    "    f = [[nltk.edit_distance(entry,poss[x][0]),Words[poss[x][1]]] for x in range(len(poss))]\n",
    "    return [sorted(f)[0][1], sorted(f)[1][1], sorted(f)[2][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Super_Lookup('Firestone-Walker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately I realized I would be better off using a open source python library called Whoosh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "#Declare the schema\n",
    "schema = Schema(beer_id=STORED(),\n",
    "                brewery=TEXT(stored=True),\n",
    "                beer=TEXT(stored=True),\n",
    "                group=KEYWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Beer_Dataset',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the index used to search\n",
    "import os.path\n",
    "from whoosh.index import create_in\n",
    "\n",
    "if not os.path.exists(\"index\"):\n",
    "    os.mkdir(\"index\")\n",
    "ix = create_in(\"index\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.index import open_dir\n",
    "\n",
    "ix = open_dir(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open writer in order to add documents to the index\n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df[['brewery','beer_id','beer','group']].groupby(['beer_id','brewery','beer','group']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(len(docs)):\n",
    "    writer.add_document(brewery = docs.brewery[t], beer = docs.beer[t], beer_id = docs.beer_id[t], group = docs.group[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#commit changes to index\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a multifield parser that allows user to search beer, brewery, or both fields\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "qp = MultifieldParser([\"brewery\", \"beer\"], schema=ix.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#an example of searching the index for the string 'fat tire amber'\n",
    "\n",
    "#create search parse\n",
    "q = qp.parse(u'fat tire amber')\n",
    "\n",
    "#search index\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    \n",
    "    #record results and format\n",
    "    liner = []\n",
    "    ids = []\n",
    "    for x in range(len(results)):\n",
    "        try:\n",
    "            be = results[x]['beer']\n",
    "            br = results[x]['brewery']\n",
    "            bi = results[x]['beer_id']\n",
    "            new_line = str(be + ', ' + br)\n",
    "            ids.append(bi)\n",
    "            liner.append(new_line)\n",
    "        except:\n",
    "            print results[x]\n",
    "\n",
    "    lines = []\n",
    "    \n",
    "    #format for use with dropdown list in plotly dash\n",
    "    for d in range(len(resulte)):\n",
    "        ent = {'label':liner[d],'value':ids[d]}\n",
    "        lines.append(ent)\n",
    "    print lines\n",
    "    \n",
    "#In the app user select the beer name from a list and press a button which adds the corresponding value, the beer_id, to\n",
    "#beer list so users don't need to manually look up the ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the app please go to: https://github.com/thewho14/Beer_Maven and follow the installation instructions in the readme.  Please be patient as the app runs very slowly at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Analsysis<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I tried a collaborative recommendation system based on ratings, I tried building a content based recommendation system\n",
    "using text reviews for the top 250 beer most rated beers on beeradvocate.com.  I'll omit the code used to scrape as it is very similar to scraping code above.  The dataset I created is a dictionary where each key is a beer name and the values are lists of strings storing reviews for that beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I first tried a bag of words approach to find top used words for each beer.\n",
    "import pickle\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revs = pickle.load(open('Beer_Reviews.pkl'))\n",
    "\n",
    "#flatten lists inside of the data\n",
    "for name in revs.keys():\n",
    "    reviews = []\n",
    "    for n in range(len(revs[name])):\n",
    "        if type(revs[name][n]) == unicode:\n",
    "            reviews.append(revs[name][n])\n",
    "        elif type(revs[name][n])==list:\n",
    "            reviews.extend(revs[name][n])\n",
    "    revs[name] = reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetTopWords(key):\n",
    "    '''takes beer name as a string and returns the top 250 most used words from reviews for that beer'''\n",
    "    #initialize count vectorizer, fit to reviews, and transform them into bag of words format\n",
    "    count_vect = CountVectorizer(stop_words = 'english',analyzer='word')\n",
    "    Mega = count_vect.fit_transform(revs[key])\n",
    "    \n",
    "    #Initialize tfid transformer and transform the bag of words\n",
    "    Tfid = TfidfTransformer()\n",
    "    Kila = Tfid.fit_transform(Mega)\n",
    "    \n",
    "    #Get index of top words\n",
    "    ind = Kila.toarray().sum(axis=0).argsort()\n",
    "    brew = ind[::-1][0:250]\n",
    "    \n",
    "    #get top words\n",
    "    b = [count_vect.get_feature_names()[n] for n in brew]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GetTopWords('Budweiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the top 250 words for each beer in a new dictionary\n",
    "top250 = defaultdict()\n",
    "\n",
    "for k in revs.keys():\n",
    "    top250[k] = GetTopWords(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a first try at recommendation.  Beers are listed in order of how many words they share from their top 250 list.\n",
    "def FindSimilarBeers(Beer):\n",
    "    '''Takes a beer name and returns beer in order of how many top words they share'''\n",
    "    counts = []\n",
    "    #pickle.load(open(top100,'wb'))\n",
    "    words = top250[Beer]\n",
    "    for k in top250.keys():\n",
    "        if k != Beer:\n",
    "            counts.append(len(words.intersection(top250[k])))\n",
    "    closest = np.argmax(counts)\n",
    "    return top250.keys()[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This approach did not work very well\n",
    "FindSimilarBeers('Sculpin IPA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many top words are not useful on their own.  They need to be included in phrases in order them to be meaningful.  For example, words like \"taste\", \"head\", and \"flavor\" need an adjective preceding them to make them meaningful ie \"bad taste\", \"tan head\", and \"malty flavor\".  As a result I moved on to looking for ways to find descriptive phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I tried used Bigram and Trigram collocations in nltk to look for common pairings, but these still weren't useful phrases\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "[revs['Bud Light']\n",
    "finder = BigramCollocationFinder.from_words(revs['Bud Light'])\n",
    "finder.nbest(bigram_measures.pmi, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I also tried using noun chunks in spaCy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse reviews in spaCy doc objects\n",
    "doc = nlp.pipe(revs['Bud Light'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out noun chunks.  As you can see this also does not work very well\n",
    "for doc in nlp.pipe(revs['Bud Light'][0:10]):\n",
    "    for k in doc.noun_chunks:\n",
    "            print k, len(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When documents are parsed in spaCy, they are assigned a part-of-speech tags based spaCy's build in deep learning model.  I decided to manually look for common patterns in these parts-of-speech tags in order to capture phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample review string\n",
    "string = (u'Pours a clear copper, with 2 ½ big foamy fingers of shiny off-white head. This shrinks down pretty quickly, with a frothy ½ finger that leaves back a cascade of lacing which hangs in sticky clumps. The aroma is floral and leafy, with a faint metallic tinge to the proceedings. The rest of the nose is filled out with citrus, caramel, toast, and a light touch of diacetyl. Off-notes hurt this a bit.' )\n",
    "\n",
    "#remove punctuation\n",
    "string = string.replace(u',','').replace(u'\\'','').replace(u'.','')\n",
    "\n",
    "#parse\n",
    "text = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "#initialize spaCy matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#define patterns such as \"adjective noun\" and \"noun noun conjunctive noun\"\n",
    "pattern1 = [{'TAG':'JJ'},{'TAG':'NN'}]\n",
    "pattern2 = [{'TAG':'JJ'},{'TAG':'JJ'}]\n",
    "pattern3 = [{'TAG':'NN'},{'TAG':'NN'}]\n",
    "pattern4 = [{'TAG':'NN'},{'TAG':'JJ'}]\n",
    "pattern5 = [{'TAG':'NN'},{},{'TAG':'NN'},{'TAG':u',','OP':'!'}]\n",
    "pattern6 = [{'TAG':'NN'},{'TAG':'NN'},{'TAG':u',','OP':'*'},{'TAG':'CC'},{'TAG':'NN'}]\n",
    "\n",
    "#match patterns\n",
    "matcher.add('type1', None, pattern1,pattern2,pattern3,pattern4,pattern5, pattern6)\n",
    "matches = matcher(text)\n",
    "\n",
    "r = [[matches[n][1],matches[n][2]] for n in range(len(matches))]\n",
    "[text[r[n][0]:r[n][1]] for n in range(len(r))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
